{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "**Stochastic gradient descen** is how errors are minimized in the preceding scenario.\n",
    "As mentioned earlier, gradient stands for the difference (which is the difference in\n",
    "loss values when the weight value is updated by a small amount) and descent means\n",
    "to reduce. Stochastic stands for the selection of random samples based on which a\n",
    "decision is taken.\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import latexify\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\begin{array}{l} pre_hidden = \\mathrm{dot}\\left(inputs, {weights_{{0}}} + {weights_{{1}}}\\right) \\\\ hidden = \\frac{{1}}{{1} + \\exp{\\left({-pre_hidden}\\right)}} \\\\ pred_out = \\mathrm{dot}\\left(hidden, {weights_{{2}}} + {weights_{{3}}}\\right) \\\\ mean_squared_error = \\mathrm{mean}\\left(\\mathrm{square}\\left(pred_out - outputs\\right)\\right) \\\\ \\mathrm{feed_forward}(inputs, outputs, weights) = mean_squared_error \\end{array} $$"
      ],
      "text/plain": [
       "<latexify.frontend.LatexifiedFunction at 0x2a57f851270>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feedforward \n",
    "@latexify.with_latex\n",
    "def feed_forward(inputs, outputs, weights):\n",
    "    pre_hidden = np.dot(inputs, weights[0] + weights[1])\n",
    "    hidden = 1/(1 + np.exp(-pre_hidden))\n",
    "    pred_out = np.dot(hidden, weights[2] + weights[3])\n",
    "    mean_squared_error = np.mean(np.square(pred_out - outputs))\n",
    "    return mean_squared_error\n",
    "\n",
    "feed_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the weightds\n",
    "@latexify.with_latex\n",
    "def update_weights(inputs, outputs, weights, lr):\n",
    "    original_weights = deepcopy(weights) # deepcopy ensures we keep the original weights as it's \n",
    "    temp_weights = deepcopy(weights)\n",
    "    updated_weights = deepcopy(weights)\n",
    "    # Now we will calcuate the original loss with orignal set of weights\n",
    "    original_loss = feed_forward(inputs, outputs, original_weights)\n",
    "    # we will loop through all the layers in the network\n",
    "    for i, layer in enumerate(original_weights):\n",
    "        # since wwe have 4 parameters, we will loop through each parameter in the list\n",
    "        for index, weight in np.ndenumerate(layer):\n",
    "            temp_weights = deepcopy(weights)\n",
    "            temp_weights[i][index] += 0.0001\n",
    "            _loss_plus = feed_forward(inputs, outputs, temp_weights)\n",
    "            # we calculate the gradient due to the weight change\n",
    "            grad = (_loss_plus - original_loss)/(0.0001)\n",
    "            # we update the parameter at ith layer and index of updated weights\n",
    "            updated_weights[i][index] -= grad*lr\n",
    "            # Once the parameter values across all layers and indices within layer are upated, we return the upated weight values\n",
    "            return updated_weights, original_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
